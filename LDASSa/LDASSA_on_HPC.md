---
author:
- |
    Gabriëlle De Lannoy, Martijn Oldenhof: version 4\
    (`gabrielle.delannoy@kuleuven.be`)
title: |
    Compiling GMAO's LDASsa, and\
    Creating AGCM-LDASsa parameters\
    on the HPC at KU Leuven
---

  ------------- -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  version 1     Bertrand Bonan, initial compilation using mpiifort
  version 1.1   Bertrand Bonan, Gabriëlle De Lannoy, updated documentation, summary
  version 2     Gabriëlle De Lannoy, compilation using OpenMPI and new libraries (LDASsa\_libraries\_20170421 for Thinking), updated documentation
  version 3     Gabriëlle De Lannoy, AGCM/LDASsa parameter creation, mkCatchParam
  version 4     Gabriëlle De Lannoy, moved from Thinking to Genius, recompiled LDASsa using mpiifort and new libraries (LDASsa\_libraries\_20190607 for Genius). New, make sure to also update ESMA\_base.mk! Updated documentation with links to new VSC webpages.
  ------------- -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\
\
Acknowledgement: this work was supported by the VSC HPC Team at KU
Leuven.

Background information
======================

This document describes how to install and run NASA/GMAO's LDASsa and
related software on the KU Leuven [Tier-2
cluster](https://vlaams-supercomputing-centrum-vscdocumentation.readthedocs-hosted.com/en/latest/leuven/tier2_hardware.html) ([pdf-file](https://docs.wixstatic.com/ugd/5446c2_2a3546e56a5349ef8d027132459e8c64.pdf)).

-   get a VSC-account and manage it: <https://account.vscentrum.be/>

-   obtain original LDASsa-tag from NASA/GMAO (POC Rolf Reichle); source
    code can be checked out on NASA's NCCS Discover cluster, where it
    was developed

-   LDASsa source code can be copied from the following folder:\
    `/data/leuven/314/vsc31402/src_code/GEOS5_LDASsa`\
    The source code is accessible to anyone in the group `lees_swm_ls`

-   scripts adapted for the HPC at KU Leuven are indicated with `_KUL`
    (environment variables and local libraries on Tier-2 Thinking), or
    with `_KUL_Genius` (environment variables and local libraries on
    Tier-2 Genius)

[To find information about the VSC
HPC](https://vlaams-supercomputing-centrum-vscdocumentation.readthedocs-hosted.com/en/latest/):
browse through <https://www.vscentrum.be/>.

LDASsa compilation {#sec:compilation}
==================

To compile LDASsa source code, open a terminal and go to the source
code. The source code (e.g. `/DASsa_m3-16_0_p1_KUL/LDASsa`) has 2
directories prior to compilation: `src` and `CVS`. If there are more
directories, clean first
(Section [2.1](#subsec:clean){reference-type="ref"
reference="subsec:clean"}). Ensure that you are using csh (or tcsh). By
default on the Tier-2 cluster, terminals use bash, csh (or tcsh) can be
used by just typing `csh` (or `tcsh`) in the terminal. The two-step
compilation procedure from the src folder is listed below:

    $ source g5_modules
    $ gmake install

csh (or tcsh) is used here because the g5\_module script was written
originally in csh and does not work properly if bash is used.

After compilation, a `Linux`-directory is created with relevant
executables. Most importantly, `/LDASsa/Linux/bin/` contains:

-   `LDASsa_assim_mpi.x`: LDASsa executable

-   `ldsetup`: (copied from `/LDASsa/src/Applications/LDAS_App/ldsetup`)
    to set up and submit a job

We cannot compile LDAS on just any random node:\
- Thinking: if you use NoMachine, compilation only works on the group
node (r12i2n16). When ssh-ing via putty (Xming) or a mac-terminal,
compilation also works on the login, group and inodes just fine.\
- Genius: libraries and variables cannot be set properly when logged in
via ssh nx-new.hpc.kuleuven.be (via putty, mac). We need to use
NoMachine, or ssh login1-tier2.hpc.kuleuven.be via putty or a
mac-terminal.

Cleaning before new compilation {#subsec:clean}
-------------------------------

In case you need to recompile the code, a good policy is to clean all
the folders by deleting the executables, object and library files
generated by the compilation. This can be done by using the following
command:

    $ gmake clean

Sometimes it may be useful to also delete dependence files built during
the compilation. This can be done using:

    $ gmake distclean

Or, to delete all the folders created during the compilation process,
use:

    $ gmake realclean

Libraries
---------

**$\Rightarrow$ Libraries, compiler, flags, environmental variables will
be sorted out centrally for all of lees\_swm\_ls; a user on the HPC does
not need to rework this.**\
Libraries, modules and environmental variables are prepared (i) in
g5\_modules (see Section [2.4](#subsec:modified){reference-type="ref"
reference="subsec:modified"}) and (ii) by properly structuring a list of
selected libraries in BASELIB (below).

-   Thinking: compilation with Intel MPI (mpiifort) works (intel/2015a
    toolchain, version 1 of this document), but multi-node simulation
    fails. Therefore,we moved to OpenMPI (mpif90) and a major overhaul
    of libraries was implemented as follows
    ([/staging/leuven/stg\_00024/LDASsa\_libraries\_20170431/](/staging/leuven/stg_00024/LDASsa_libraries_20170431/)),
    and summarized in
    Table [\[tab\_libs\]](#tab_libs){reference-type="ref"
    reference="tab_libs"}.

-   Genius: compilation with Intel MPI (mpiifort) and simulation
    succeeded.

The structure of libraries on the HPC is different from that on NASA
NCCS and therefore needs particular attention.
Table [\[tab\_libs\]](#tab_libs){reference-type="ref"
reference="tab_libs"} gives an overview of the libraries linked into the
BASELIB, for Thinking and Genius. A script is prepared to link all the
needed files to the proper location BASELIB. For Thinking, use
[/staging/leuven/stg\_00024/LDASsa\_libraries\_20170421/script\_get\_baselib](/staging/leuven/stg_00024/LDASsa_libraries_20170421/script_get_baselib);
for Genius, use
[/staging/leuven/stg\_00024/LDASsa\_libraries\_20190607/script\_get\_baselib](/staging/leuven/stg_00024/LDASsa_libraries_20190607/script_get_baselib):

-   `mkdir LDASsa_libraries_20170421`: create new BASELIB directory

-   `cd LDASsa_libraries_20170421; mkdir Linux`: create empty directory
    'Linux'

-   `csh script_get_baselib Linux`: organize libraries as expected by
    the makefiles

For Thinking, the libraries are taken from
[/apps/leuven/thinking/2015a/software/](/apps/leuven/thinking/2015a/software/);
for Genius, the libraries are taken from
[/apps/leuven/skylake/2018a/software/](/apps/leuven/skylake/2018a/software/).

Updates to `script_get_baselib` (20190607-version) to work on Genius:
(i) other libraries (see
Table [\[tab\_libs\]](#tab_libs){reference-type="ref"
reference="tab_libs"}), (ii) linking netcdff (double f) into the
`BASELIB/Linux/include`, and (iii) adding nf-config next to nc-config to
the `BASELIB/Linux/bin`. The later two overcome the issue reported in
Section [2.5.2](#subsubsec_HDF){reference-type="ref"
reference="subsubsec_HDF"} for Thinking.

              Thinking                           Genius
  -------------------------------- -----------------------------------
       cmor/3.2.0-intel-2015a              cmor/3.2.0 $^{(1)}$
      cURL/7.43.0-intel-2015a           cURL/7.58.0-GCCcore-6.4.0
      ESMF/7.0.0-iomkl-2015.01      ESMF/7.1.0r-intel-2018a $^{(2)}$
      HDF/4.2.11-iompi-2015.01          HDF/4.2.14-GCCcore-6.4.0
      HDF5/1.8.9-iompi-2015.01           HDF5/1.10.1-intel-2018a
   libjpeg-turbo/1.4.0-intel-2015   libjpeg-turbo/1.5.3-GCCcore-6.4.0
     netCDF/4.1.3-iompi-2015.01         netCDF/4.6.0-intel-2018a
        Szip/2.1-intel-2015a            Szip/2.1.1-GCCcore-6.4.0
     UDUNITS/2.1.24-intel-2015a        UDUNITS/2.2.26-intel-2018a
   util-linux/2.26.1-intel-2015a     util-linux/2.31.1-GCCcore-6.4.0
       zlib/1.2.8-intel-2015a           zlib/1.2.11-GCCcore-6.4.0

  : Libraries linked into a local BASELIB directory.
  [\[tab\_libs\]]{#tab_libs label="tab_libs"}

$^{(1)}$ cmor was not available, Martijn compiled this. However, note
that it was set up with a uuid-dependency that is not perfectly in line
with the used util-linux in BASELIB, i.e.
`./configure [...cmor] -with-uuid=/apps/leuven/skylake/2018a/software/uuid/1.6.2`
(consider changing this in the future to ensure consistency, i.e. use
the same uuid files for cmor and for util-linux).\
$^{(2)}$ ESMF was not available, Martijn compiled this and he linked in
the missing filles mentioned under
Section [2.5.1](#subsubsec:ESMF){reference-type="ref"
reference="subsubsec:ESMF"}.

GNUmakefile
-----------

The key files that set compiler rules and flags, and ultimately compile
LDASsa:

-   [LDASsa/src/GNUmakefile](LDASsa/src/GNUmakefile): contains
    references to files below, invokes a recurse make in sub-directories

-   [LDASsa/src/Config/ESMA\_base.mk](LDASsa/src/Config/ESMA_base.mk):
    sets library-paths, include-files, and determines how each type of
    file (\*.F90, \*.c, \...) needs to be compiled (rules)

-   [LDASsa/src/Config/ESMA\_arch.mk](LDASsa/src/Config/ESMA_arch.mk):
    further sets paths and variables for each type of architecture (in
    our case on Genius, using Intel Fortran compiler)

-   [LDASsa/src/Config/GNUmakefile](LDASsa/src/Config/GNUmakefile):
    compiling ancillary stuff, not so important

-   [LDASsa/src/Applications/GNUmakefile](LDASsa/src/Applications/GNUmakefile):
    compiling the land model

-   [LDASsa/src/Applications/LDAS\_App/GNUmakefile](LDASsa/src/Applications/LDAS_App/GNUmakefile):
    compiling the land model

Modified files for compilation on Genius {#subsec:modified}
----------------------------------------

**$\Rightarrow$ Need to be updated by each individual of
lees\_swm\_ls**\
After obtaining (a new tag of) LDASsa code from NCCS, you should update
the following files, to obtain the correct libraries and modules, etc.
Additions are noted in [blue]{style="color: blue"} and deletions in
[red]{style="color: red"}.

### g5\_modules

\(1) Deleted at line 81:

`else`\ `if`\ `(($node`\ `= `\ `pfe*)`\ `||`\ `($node`\ `= `\ `p4fe*)`\ `||`\ `\`\
[\ `($node`\ `= `\ `r[0-9]*i[0-9]*n[0-9]*)`\ `||`\ `\`]{style="color: red"}\
`($node`\ `= `\ `bridge*))`\ `then`\
`set`\ `site`\ `=`\ `"NAS"`

Added at line 94:

`else`\ `if`\ `(`\ `$node`\ `= `\ `*niteroi*`\ `)`\ `then`\
`set`\ `site`\ `=`\ `"GMAO.niteroi"`

[`else`\ `if`\ `(`\ `($node`\ `= `\ `r[0-9]*i[0-9]*n[0-9]*)`\ `||`\ `($node`\ `= `\ `hpc*)`\ `||`\ `\`\
`(`$node =~ login*) || ($`node`\ `= `\ `tier*)`\ `)`\ `then`\
`set`\ `site`\ `=`\ `"HPC.KULeuven"`]{style="color: blue"}

`else`\ `if`\ `(`\ `-d`\ `/ford1/share/gmao_SIteam/`\ `&&`\ `-d`\ `/ford1/local/`\ `&&`\ `$arch`\ `==`\ `Linux`\ `)`\ `then`\
`set`\ `site`\ `=`\ `"GMAO.desktop"`

\(2) Added at line 211:

::: {style="color: blue"}
`#=================#`\
`#`\ `HPC`\ `KU`\ `Leuven`\ `#`\
`#=================#`\
`else`\ `if`\ `(`\ `$site`\ `==`\ `HPC.KULeuven`\ `)`\ `then`

`if`\ `($VSC_INSTITUTE_CLUSTER`\ `= `\ `genius)`\ `then`\ `#Genius`\
`set`\ `basedir`\ `=`\ `/staging/leuven/stg_00024/LDASsa_libraries_20190607`\
`set`\ `mod1`\ `=`\ `Python/2.7.14-intel-2018a`\
`set`\ `mods`\ `=`\ `(`\ `$mod1`\ `)`\
`else`\ `#Thinking`\
`set`\ `basedir`\ `=`\ `/staging/leuven/stg_00024/LDASsa_libraries_20170421`\
`set`\ `mod1`\ `=`\ `iomkl/2015.01`\
`set`\ `mod2`\ `=`\ `OpenMPI/1.10.2-iccifort-2015.1.133-GCC-4.9.2`\
`set`\ `mod3`\ `=`\ `Python/2.7.9-iomkl-2015.01`\
`set`\ `mods`\ `=`\ `(`\ `$mod1`\ `$mod2`\ `$mod3`\ `)`\
`set`\ `modinit`\ `=`\ `/usr/share/Modules/init/csh`\
`endif`

`set`\ `loadmodules`\ `=`\ `0`\
`set`\ `usemodules`\ `=`\ `0`
:::

\(3) Added at line 292:

`if`\ `($?basedir)`\ `then`\
`if`\ `(!`\ `$wrapper)`\ `echo`\ `-n`\ `"$``scriptname``:`\ `Setting`\ `BASEDIR"`\
`setenv`\ `BASEDIR`\ `$basedir`\
[`if`\ `($site`\ `==`\ `HPC.KULeuven)`\ `then`\
`if`\ `($VSC_INSTITUTE_CLUSTER`\ `= `\ `genius)`\ `then`\ `#Genius`\
`#`\ `do`\ `nothing`\ `–`\ `#module`\ `use`\ `/apps/leuven/skylake/2018a/modules/all`\
`else`\ `#Thinking`\
`source`\ `/apps/leuven/bin/switch_to_2015a`\
`endif`\
`endif`\
]{style="color: blue"}

\(4) Added at line 352:

`if`\ `(-e`\ `$modinit)`\ `then`\
`[...]`

::: {style="color: blue"}
`else`

`#GDL`\ `20190517`\
`if`\ `((!`\ `$wrapper)`\ `&&`\ `$site`\ `==`\ `HPC.KULeuven)`\ `then`\
`if`\ `(`\ `$VSC_INSTITUTE_CLUSTER`\ `= `\ `genius`\ `)`\ `then`\
`foreach`\ `mod`\ `(`\ `$mods`\ `)`\
`module`\ `load`\ `$mod`\
`end`\
`endif`\
`endif`

`endif`
:::

\(5) Added at line 380:

`echo`\ `’eval`\ ''`$modulecmd`\ `sh`\ `purge’`''\ `>>`\ `$outfil`\
[`if`\ `($site`\ `==`\ `HPC.KULeuven`\ `&&`\ `($VSC_INSTITUTE_CLUSTER`\ `= `\ `thinking))`\ `then`\
`echo`\ `’eval`\ `‘/usr/bin/modulecmd`\ `sh`\ `unuse`\ `/apps/leuven/thinking/2014a/modules/all`''\ `>>`\
`$outfil`\
`echo`\ `’eval`\ `‘/usr/bin/modulecmd`\ `sh`\ `unuse`\ `/apps/leuven/thinking/2016a/modules/all`''\ `>>`\
`$outfil`\
`echo`\ `’eval`\ `‘/usr/bin/modulecmd`\ `sh`\ `use`\ `/apps/leuven/thinking/2015a/modules/all`''\ `>>`\
`$outfil`\
`endif`]{style="color: blue"}\
`if`\ `($loadmodules)`\ `then`\
`echo`\ `’eval`\ ''`$modulecmd`\ `sh`\ `load`\ `modules’`''\ `>>`\ `$outfil`

These edits work either on genius and thinking nodes.

### /Config/ESMA\_base.mk

Modified at line 298:

[`...`\ `$(CPP)`\ `-C`\ `-ansi`\ `-DANSI_CPP`\ `$(FPPFLAGS)`\ `>`\ `$___.s90`\ ]{style="color: red"}\
[`...`\ `fpp`\ `-C`\ `-std=gnu11`\ `-nostdinc`\ `$(FPPFLAGS)`\ `>`\ `$___.s90`]{style="color: blue"}

This specific edit is needed to work on genius nodes.

### /Config/ESMA\_arch.mk

\(1) Modified at line 166:

[`OMPFLAG`\ `:=`\ `-openmp`]{style="color: red"}\
[`OMPFLAG`\ `:=`\ `-qopenmp`]{style="color: blue"}

\(2) Modified at line 315:

`#`\ `MKL`\ `math`\ `library`\
`#`\ `—————-`\
[`MKLPATH`\ `=`\ `$(MKLROOT)/lib/intel64`]{style="color: blue"}\
[`ifeq`\ `($(wildcard`\ `$(ESMABIN)/mklpath.pl),$(ESMABIN)/mklpath.pl)`\
`MKLPATH`\ `=`\ `$(shell`\ `$(ESMABIN)/mklpath.pl)`\
`endif`\
`ifdef`\ `MKLPATH`]{style="color: red"}\
`INC_SCI`\ `+=`\ `-I$(MKLROOT)/include/intel64/lp64`\ `-I$(MKLROOT)/include`\
`LIB_SCI`\ `+=`\ `$(MKLPATH)/libmkl_lapack95_lp64.a`\ `-Wl,`\
`–start-group`\ `$(MKLPATH)/libmkl_intel_lp64.a`\ `$(MKLPATH)/libmkl_sequential.a`\
`$(MKLPATH)/libmkl_core.a`\ `-Wl,–end-group`\ `-lpthread`\ `-lm`\
[`endif`]{style="color: red"}

Comment: The Perl script `mklpath.pl` invoked in the Makefile is
supposed to set `MKLROOT` and `MKLPATH` correctly. For some reason,
`MKLROOT` is set correctly but `MKLPATH` is not. Therefore `MKLPATH` is
now hardwired in `ESMA_arch.mk` directly. The need for this edit is not
specifically checked on genius-nodes and simply inherited from working
on thinking-nodes (e.g. the necessity of the second edit above is not
verified).

### /Config/GMAO\_base.mk {#sec:GMAObase}

The very last lines are removed:

::: {style="color: red"}
`#`\ `—————–`\
`#`\ `Check`\ `Environment`\
`#`\ `—————–`

`ifneq`\ `(`$(shell$`(ESMABIN)/Assert),`\ `0)`\
`$(error`\ `Please`\ `correct`\ `your`\ `build`\ `environment`\ `and`\ `try`\ `again)`\
`endif`
:::

Comment: good grief, I have no idea why! The Assert probably does not
work? But this is a risky hack\... The need for this edit is not
specifically checked on genius-nodes and simply inherited from working
on thinking-nodes.

### /Applications/LDAS\_App/GNUmakefile {#sec:MakefileHDF}

Added at line 122:

`MY_LIBS`\ `=`\ `$(LIB_ESMF)`\ `$(MYLIB_GFIO)`\ `\`\
`$(LIB_MFHDF3)`\ `$(LIB_SDF)`\ `\`\
`$(LIB_SYS)`\ `$(LIB_HDF5)`\ [`$(LIB_HDF)`]{style="color: blue"}

Comment: HDF library is needed here to produce the executable
`LDASsa_assim_mpi.x`, inherited from compiling on thinking-nodes.

### /Components/GEOSlana\_GridComp/easeV1\_conv.F90

Modified at line 27:

[\ `!`\ `-`\ `clean-up`\ `of`\ `GEOSlana_GridComp/*.F90`\ `files:`]{style="color: red"}\
[\ `!`\ `-`\ `clean-up`\ `of`\ `F90`\ `files`\ `in`\ `GEOSlana_GridComp/`\ `folder:`]{style="color: blue"}

Comment: The comment at line 27 threw up an error at compilation, but
got ignored after all. The compiler detects the opening of a block
comment with `/*`; just by rewriting the comment, the issue is resolved,
inherited from compiling on thinking-nodes.

### /Components/GEOSlana\_GridComp/clsm\_ensdrv\_pert\_routines.F90

Modified at line 2866:

[`real,`\ `dimension(:,:),`\ `pointer,`\ `intent(out)`\ `::`\ `Pert_ntrmdt_f`\ ]{style="color: red"}\
[`real,`\ `dimension(:,:),`\ `pointer,`\ `intent(inout)`\ `::`\ `Pert_ntrmdt_f`]{style="color: blue"}

Comment: the code compiled just fine with the original line, but the
ensemble simulation went wrong and ended with a segmentation fault.

Error messages encountered during installation {#sec:errmess}
----------------------------------------------

The following problems required some fixing on the KU Leuven HPC prior
to a successful compilation.

### Missing files in ESMF libraries {#subsubsec:ESMF}

        ESMF\_ErrReturnCodes.inc: No such file or directory

Diagnostic: The install target in the Makefile for ESMF installation is
broken. As a consequence several inc files are not copied in the include
folder.\
Solution: HPC staff manually installed (just copy in the directory
`/apps/leuven/thinking/2015a/software/ESMF/7.0.0-intel-2015a/include`,
for Thinking, or
`/apps/leuven/skylake/2018a/software//ESMF/7.1.0r-intel-2018a/include/`
for Genius) the following files: `ESMF_ErrReturnCodes.inc`,
`ESMF_InitMacros.inc`, `ESMF_LogConstants.inc`, `ESMF_LogMacros.inc`,
`EMSF_Macros` and `ESMF_TimeMgr.inc`.

### NetCDF and HDF library {#subsubsec_HDF}

Only on Thinking:

    Undefined reference to hdf-related functions, e.g., ``hopen, vfstart ...''

Solution: See file `./Applications/LDAS_App/GNUmakefile`, and make sure
that a good NetCDF and HDF library is used (done by sysadmins in
response to issues somewhat confusingly reported in version 1.1, section
3.1.2). These issues did not appear anymore with the new setup on
genius-nodes.

LDASsa runs {#sec:run}
===========

How to?
-------

To run a job, we need [HPC
credits](https://vlaams-supercomputing-centrum-vscdocumentation.readthedocs-hosted.com/en/latest/jobs/credit_system_basics.html#credit-system-basics):
click on link for more info.\
The Python script `ldsetup` located in the folder `./Linux/bin/` is here
to help you set up or submit a job on the HPC
(Section [3.4](#subsec_ldsetup){reference-type="ref"
reference="subsec_ldsetup"}).\
Before submitting a job with `ldsetup`, you need to check if the script
`.g5_modules.sh` exists in the folder `./Linux/bin`. This script in bash
will be used to load the various modules needed on the computational
nodes to run LDASsa (computational nodes works with bash by default).
The script can be generated by opening a csh terminal and by using the
following commands:

    $ source g5_modules
    $ ./g5_modules sh

    $ ldsetup setup ./output_directory ./input.exeinp ./input.batinp --verbose

This is particularly useful, if you don't want to submit your job to the
queue. The file `./output_direcotry/run/lenkf.0.j` contains the
mpirun-command with the necessary arguments: using this command (without
the backslash separators) on an interactive node will start the
simulation from the command line. Another useful application is to enter
the arguments from the mpirun into a debugger interface.\
**A job is submitted to the queue using the command**

    $ ldsetup setup ./output_directory ./input.exeinp ./input.batinp --verbose --submit

-   `./output_directory` is the path to the output directory. The
    directory needs to be created beforehand and has to be empty (no
    overwriting of existing files is allowed).

-   `./input.exeinp` is the file that contains the input arguments used
    by LDASsa.

-   `./input.batinp` is the file that contains practical information for
    the job that need to be filled with caution.

-   `–submit` is the flag that commands the job submission.

Compute nodes are bash-only, the `ldsetup`-script looks for
`g5_modules.sh`, so make sure it is available (see above)!

, either manually enter the information in ddt or as follows on the
prompt (stay in bash, not csh!):

-   `qsub -I -lnodes=2:ppn=20 -lwalltime=00:20:00`

-   when you have the interactive node:\
    `>> echo $PBS_NODEFILE`\
    `>> /var/spool/torque/aux//20499919.tier2-p-moab-1.icts.hpc.kuleuven.be`

-   `cp $PBS_NODEFILE /scratch/...[experiment]/run/.`

-   when you have the interactive node: `>> ssh -X i0r5n4`: open new
    xterm to circumvent the memory limit (for now)

-   (Thinking)

-   (Genius)

-   `source ../build/Linux/bin/.g5_modules.sh`

-   ` mpirun -np 20 -machinefile ./20500283.tier2-p-moab-1.icts.hpc.kuleuven.be /scratch/leuven/314/vsc31402/output/TEST_RUNS_16/GLOB_EASEv2_M09_N_2n20p/build/Linux/bin/LDASsa_mpi.x -work_path ../output -run_path . -exp_id GLOB_EASEv2_M09_N_2n20p -exp_domain SMAP_EASEv2_M09_GLOB -start_year 1985 -start_month 1 -start_day 1 -start_hour 0 -start_min 0 -start_sec 0 -end_year 1985 -end_month 10 -end_day 1 -end_hour 0 -end_min 0 -end_sec 0 -N_ens 1 -spin .false. -force_dtstep 3600 -first_ens_id 0 -resolution SMAP_EASEv2_M09 -restart .false. -met_tag M2COR_cross -met_path ../input/met_forcing/MERRA2_land_forcing -driver_inputs_path . -driver_inputs_file drv_EASEv2_M09_GLOB.nml`

Make sure to `source .g5_modules.sh`, before running ddt.

./input.exeinp
--------------

The file `./input.exeinp` contains the input arguments needed by LDASsa
for the run. The role of each argument is detailed below.

    # REQUIRED inputs
    # These fields are needed to set up output dir structure.
    # Format for start/end times is yyyy-mm-dd-hh-mm-ss.
    #
    exp_id                  = US_M36_EASEv2_atL_DA_7Thv_CalDnew_M
    exp_domain              = SMAP_EASEv2_M36_US
    N_ens                   = 24
    start_time              = 2010-01-01-00-00-00
    end_time                = 2010-02-01-00-00-00

-   `exp_id` is the name of the experiment. The directory
    `./output_directory/exp_id/` will contain all the output files (copy
    of input files and results) produced during the experiment.

-   `exp_domain` describes the domain used by the run. For example
    `SMAP_EASEv2_M36_US` designates the EASEv2 grid with a 36-km
    resolution over the US. Results of the experiments are stored in the
    directory `./output_directory/exp_id/output/exp_domain/`

-   `N_ens` is the size of the ensemble (number of members in the
    ensemble).

-   `start_time` is the time at the beginning of the experiment.

-   `end_time` is the time at the end of the experiment.

<!-- -->

    # OPTIONAL inputs
    # Values of these optional fields are passed to the executable
    # as is (so spin's value should be .true./.false. instead of
    # True/False etc.) and in the same order as they appear in
    # the input file. 3 keys - exp_id, work_path and run_path are
    # not allowed in input file. If they are specified, an
    # exception is raised.
    #
    spin                    = .false.
    force_dtstep            = 3600
    first_ens_id            = 0
    resolution              = SMAP_EASEv2_M36
    restart                 = .true.
    restart_path            = /staging/leuven/stg_00024/output/EASEv2_atlaunch_001/
                              GLOB_M36_EASEv2_atlaunch_001_spin_M/
    restart_domain          = SMAP_EASEv2_M36_GLOBAL
    restart_id              = GLOB_M36_EASEv2_atlaunch_001_spin_M
    met_tag                 = d5_merra_cross__GEOSdas-2_1_4
    met_path                = /staging/leuven/stg_00024/input/met_forcing/
                              MERRA_land_forcing/
    ens_prop_inputs_path    = /data/leuven/314/vsc31402/projects/TEST_RUNS/
    ens_prop_inputs_file    = ens_prop_Tb3DDA_j.nml
    driver_inputs_path      = /data/leuven/314/vsc31402/projects/TEST_RUNS/
    driver_inputs_file      = drv_964x406US_atL_3h_calDnew.nml
    ens_upd_inputs_path     = /data/leuven/314/vsc31402/projects/TEST_RUNS/
    ens_upd_inputs_file     = ensupd_SMOS_7Thv_DA_3h_M.nml

-   `spin` is a boolean (`.true.` or `.false.`) determining if we spin
    up the model at initialization or not. The spinup is currently
    hardwired to run 10 loops.

-   `force_dtstep`, in seconds, is the time step of the forcing used by
    LDASsa.

-   `first_ens_id` integer

-   `resolution` is the grid used for the experiment

-   `restart` is a boolean determining if the run start from a file
    whose path is obtained with `restart_path`, `restart_domain` and
    `restart_id`.

-   `met_tag` is a string that contains the elements needed to build the
    names of the files containing meteorological forcing (filenames are
    built in
    `./src/Components/GEOSlana_GridComp/clsm_ensdrv_force_routines.F90`).

-   `met_path` is the path where the files containing the meteorological
    forcing are located.

-   `ens_prop_inputs_path` is the path where is located the namelist
    `ens_prop_inputs_file`.

-   `ens_prop_inputs_file` contains the specification for model error
    (forecast step)

-   `driver_inputs_path` is the path where is located the namelist
    `driver_inputs_file`.

-   `driver_inputs_file` contains the specification for the model
    (vegetation, \...) and for the output.

-   `ens_upd_inputs_path` is the path where is located the namelist
    `ens_upd_inputs_file`.

-   `ens_upd_inputs_file` contains the specification for the Ensemble
    Kalman Filter (analysis step *e.g.* update).

./input.batinp
--------------

    # ------
    # Inputs for resource manager (SLURM, PBS etc.)
    # ------
    #
    # REQUIRED inputs
    # NOTE:
    # (1) rm_name is one of SLURM, PBS
    # (2) walltime is in the format hh:mm:ss
    #
    rm_name = PBS
    account = default_project
    walltime = 00:05:00
    ntasks = 24
    nodes = 1
    ppn = 24:haswell
    email = first_name.last_name@kuleuven.be

-   `rm_name` is the name of the job scheduler (either SLURM or PBS).

-   `account` is the account from which the credits are

-   `walltime` is the time you expect to need for the job.

-   `ntasks` is the total number of MPI processors required in mpirun.

-   `nodes` is the total number of nodes required to perform the job.

-   `ppn` is the number of processors per nodes required for the job, on
    ivybridge, haswell, or skylake (increasingly expensive).

-   `email` is the email address where a message will be sent to inform
    you when the job starts and finishes.

One modified file: ./Applications/LDAS\_App/ldsetup {#subsec_ldsetup}
---------------------------------------------------

Enable PBS queueing by the following edits.

\(1) Added at line 69:

`#`\ `——`\
`#`\ `Optional`\ `resource`\ `manager`\ `input`\ `fields`\
`#`\ `——`\
`optSlurmInpKeys`\ `=`\ `[’job-name’,`\ `’qos’,`\ `’export’,`\ `’partition’,`\ `’constraint’]`\
`optPbsInpKeys`\ `=`\ `[`[`’nodes’,`\ `’email’,`\ `’ppn’`]{style="color: blue"}`]`

Added at line 629:

`if`\ `_rm_name==’SLURM’:`\
`...`\
`directives`\ `+=`\ `’#SBATCH`\ `–error=%s\``n``’`\ `%`\ `errfile`\
[`elif`\ `_rm_name==’PBS’:`\
`directives`\ `=`\ `”`\
`#`\ `REQUIRED`\ `directives`\ `time/nodes`\
`directives`\ `+=`\ `’#PBS`\ `-l`\ `walltime=%s\``n``’`\ `%`\ `self.rqdRmInp[’walltime’]`\
`directives`\ `+=`\ `’#PBS`\ `-l`\ `nodes=%s’`\ `%`\ `self.optRmInp[’nodes’]`\
`directives`\ `+=`\ `’:ppn=%s\``n``’`\ `%`\ `self.optRmInp[’ppn’]`\
`directives`\ `+=`\ `’#PBS`\ `-A`\ `%s\``n``’`\ `%`\ `self.rqdRmInp[’account’]`\
`directives`\ `+=`\ `’#PBS`\ `-m`\ `abe\``n``’`\
`directives`\ `+=`\ `’#PBS`\ `-M`\ `%s\``n``’`\ `%self.optRmInp[’email’]`\
`#`\ `out/err`\ `files`\ `in`\ `rc_out`\
`myDateTime`\ `=`\ `’%04d%02d%02d_%02d%02d’`\ `%`\ `\`\
`(start.year,`\ `start.month,`\ `start.day,`\ `start.hour,`\ `start.minute)`\
`outfile`\ `=`\ `os.path.relpath(`\
`’/’.join([`\
`self.wrkdir,`\
`self.rqdExeInp[’exp_domain’],`\
`’rc_out’,`\
`’Y%04d’`\ `%`\ `start.year,`\
`’M%02d’`\ `%`\ `start.month,`\
`’.’.join([expid,`\ `’ldas_log’,`\ `myDateTime,`\ `’txt’]),`\
`]),`\
`self.rundir)`\
`errfile`\ `=`\ `os.path.relpath(`\
`’/’.join([`\
`self.wrkdir,`\
`self.rqdExeInp[’exp_domain’],`\
`’rc_out’,`\
`’Y%04d’`\ `%`\ `start.year,`\
`’M%02d’`\ `%`\ `start.month,`\
`’.’.join([expid,`\ `’ldas_out’,`\ `myDateTime,`\ `’txt’]),`\
`]),`\
`self.rundir)`\
`directives`\ `+=`\ `’#PBS`\ `-o`\ `%s\``n``’`\ `%`\ `outfile`\
`directives`\ `+=`\ `’#PBS`\ `-e`\ `%s\``n``’`\ `%`\ `errfile`\ ]{style="color: blue"}\
`else:`\
`raise`\ `Exception`

Comment: ldsetup is adapted in order to include PBS as a job scheduler
(PBS is the job scheduler available on Tier-2).\
(2) Modified at line 703:

`jobfile`\ `=`\ `self.rundir`\ `+`\ `’/’`\ `+`\ `jobname`\
`fout`\ `=`\ `open(jobfile,`\ `’w’)`\
[`fout.write(’#!/bin/csh`\ `-fx\``n``\``n``’);`]{style="color: red"}\
[`fout.write(’#!/bin/bash\``n``’)`\ `;`]{style="color: blue"}\
`#`\ `resource`\ `manager`\ `directives`\
`directives`\ `=`\ `self._getRMdirectives(_start_seg)+’\``n``’`\
`fout.write(directives)`\
`#`\ `set`\ `environment`\
[`fout.write(’cd`\ `$PBS_O_WORKDIR\``n``’)`]{style="color: blue"}\
[`fout.write(’limit`\ `stacksize`\ `unlimited\``n``’)`]{style="color: red"}\
[`fout.write(’ulimit`\ `-s`\ `unlimited\``n``’)`]{style="color: blue"}\
[`fout.write(’source`\ `%s/g5_modules\``n``’`\ `%`\ `os.path.relpath(bindir,self.rundir))`]{style="color: red"}\
[`fout.write(’source`\ `%s/.g5_modules.sh\``n``’`\ `%`\ `os.path.relpath(bindir,self.rundir))`]{style="color: blue"}\
[`fout.write(’setenv`\ `MKL_CBWR`\ `SSE4_2`\ `#`\ `ensure`\ `zero-diff`\ `across`\ `archs\``n``’)`]{style="color: red"}\
[`fout.write(’export`\ `MKL_CBWR=SSE4_2`\ `#`\ `ensure`\ `zero-diff`\ `across`\ `archs\``n``’)`]{style="color: blue"}\
[`fout.write(’setenv`\ `MV2_ON_DEMAND_THRESHOLD`\ `8192`\ `#`\ `MVAPICH2\``n``’)`]{style="color: red"}\
[`fout.write(’export`\ `MV2_ON_DEMAND_THRESHOLD=8192`\ `#`\ `MVAPICH2\``n``’)`]{style="color: blue"}\
`#`\ `run`\ `cmd`\
`runcmd`\ `=`\ `self._getRunCommand(iseg,`\ `_start_seg,`\ `_end_seg)`

Comment: By default terminals use bash (including submitted jobs) on
Tier-2 (instead of csh). I have modified ldsetup accordingly.\
(3) Modified at line 748:

`if`\ `rm_name==’slurm’:`\
`cmd`\ `=`\ `’sbatch’`\
`if`\ `jobid:`\
`cmd`\ `+=`\ `’`\ `–dependency=afterok:%s’`\ `%`\ `jobid`\
`cmd`\ `+=`\ `’`\ `%s’`\ `%`\ `jobscrpt`\
`elif`\ `rm_name==’pbs’:`\
[`raise`\ `Exception(’job`\ `submission`\ `unavailable`\ `for`\ `[%s]’`\ `%`\ `rm_name)`]{style="color: red"}\
[`cmd`\ `=`\ `’qsub`\ `%s’`\ `%jobscrpt`]{style="color: blue"}\
`else:`\
`raise`\ `Exception(’rm`\ `[%s]`\ `not`\ `recognized’`\ `%`\ `rm_name)`

Error messages encountered during job submission {#sec:errmess}
------------------------------------------------

The following problems required some fixing prior to simulation
(specific run cases).

### Path to restart files not found

        Error during initialization [GDL -- need exact err message]

Diagnostic: A path to a restart simulation is sought, even if restart =
.false.\
Solution: I have slightly modified `ldsetup` to take into account if the
FLAG `restart = .true.` or `.false.` in `input.exeinp` file. Before the
change, if `restart = .true.`, `ldsetup` was still looking for
`restart_path` and `restart_domain`. By modifying `ldsetup` at L. 458
(in blue):

`if`\ `’restart’`\ `in`\ `self.optExeInp:`\
`if`\ `self.optExeInp[’restart’]`[\ `==`\ `’.true.’`]{style="color: blue"}`:`

this avoids considering `restart_path` and `restart_domain` when we do
not need them.

### Other issues

None reported yet.

LDASsa: Some random notes, lessons learned
==========================================

From the attempt to compile on thinking-nodes:

-   hdf-netcdf library problem: need HDF-library without netCDF-support,
    but with Fortran enabled $\Rightarrow$ new library installed:
    `HDF/4.2.11-intel-2015a` for compilation with mpiifort

-   netcdf: older versions had fortran and C together, whereas newer
    versions have separate builds $\Rightarrow$ new library installed:
    `netCDF/4.1.3-intel-2015a` for compilation with mpiifort

-   SDPToolkit - part of GMAObaselibs on NCCS, not used at KUL

-   cdTime - taken from cmor at KUL\
    (cdunfpp-directory not properly linked, but maybe this is not even
    needed)

-   hdfeos - part of GMAObaselibs on NCCS, not used at KUL

-   `ldd /staging/leuven/stg_00024/LDASsa_libraries_20170414/Linux/lib/libhdf5.so`
    $\rightarrow$ shows which libraries are linked to this library, and
    includes ifort, which makes it not suitable for compilation with
    OpenMPI

From the attempt to compile on genius-nodes:

-   ESMA\_base.mk:\
    L.207 `CC = icc` reset to old `gcc`,\
    L.209 `CPP = fpp -std=gnu11 -nostdinc -C ` reset to old `cpp`\
    Preprocessor issues are now resolved by hardwiring the rule for
    \*.P90-files

-   ESMA\_arch.mk: `mpiifort -fpp` reset to `mpiifort` (see above)

-   at some point, we needed to set this env variable prior to
    compilation (b/c MAPL\_generic.h was not found):
    `$ setenv CPATH [path to MAPL_base]:$CPATH`

-   fyi execute `./nc_config –libs` vs `./nc_config –flibs`

Credit system

    $ module load accounting
    $ mam-balance
    $ gquote -q q24h -l nodes=1:ppn=20:ivybridge
    $ mam-statement (-a lp\_ees\_swm\_ls\_001 --summarize)

Submitting and managing jobs

    $ qsub -q q1h script\_w\_PBS\_commands.sh
    $ qsub -I -X -l walltime=2:00:00 -l nodes=1:ppn=20
       (-X only works after $ ssh -X login1/2, not from login3/4(nx) on Genius)
    $ qstat (-q)
    $ showq
    $ checkjob [jobid]
    $ qdel [jobid]
    $ showstart [jobid]

LDASsa: in a nutshell
=====================

To compile fresh code from NASA's Discover on the KU Leuven HPC:

\(1) Look for an earlier compiled KUL\_Genius-updated LDASsa version on
the HPC, and (2) adjust your own newly checked out files under
`/LDASsa/src/`:

-   [`csh`]{style="background-color: yellow"} (compilation scripts are
    all for csh only - make a customized `\sim/.cshrc` and corresponding
    `\sim/.aliases_csh` if you have none yet)

-   `cp g5_modules` from a KUL\_Genius-updated version into your own
    `/src/`

-   [`source g5_modules`]{style="background-color: yellow"}

-   `cp /Config/ESMA_base.mk` and `cp /Config/ESMA_arch.mk` from a\
    KUL\_Genius-updated version into your own `/src/Config/`

-   `cp /Components/GEOSlana_GridComp/easeV1_conv.F90` from a
    KUL\_Genius-updated version into your own
    `/src/Components/GEOSlana_GridComp/` \[optional\]

-   `cp /Components/GEOSlana_GridComp/clsm_ensdrv_pert_routines.F90`
    from a\
    KUL\_Genius-updated version into your own
    `/src/Components/GEOSlana_GridComp/`

-   `cp /Applications/LDAS_App/GNUmakefile` from a KUL\_Genius-updated
    version into your own `/src/Applications/LDAS_App/`

-   `cp /Applications/LDAS_App/ldsetup` from a KUL\_Genius-updated
    version into your own `/src/Applications/LDAS_App/`

-   [`gmake install`]{style="background-color: yellow"}

-   the executable should show up in:
    `LDASsa/Linux/bin/LDASsa_assim_mpi.x`

To move from Thinking to Genius, copy 4 files from an
KUL\_Genius-updated LDASsa version:

1.  `LDASsa/src/g5_modules`

2.  `LDASsa/src/Config/ESMA_base.mk`

3.  `LDASsa/src/Config/ESMA_arch.mk`

4.  `cp /Components/GEOSlana_GridComp/clsm_ensdrv_pert_routines.F90`
    from a KUL\_Genius-updated version into your own
    `/src/Components/GEOSlana_GridComp/`

To run:

Go to `/LDASsa/Linux/bin/`, created upon compilation:

-   [`source g5_modules`]{style="background-color: yellow"}

-   [`./g5_modules sh`]{style="background-color: yellow"}

-   [`ldsetup setup output_path project_path/input.exeinp`]{style="background-color: yellow"}\
    [`project_path/input.batinp –submit –verbose`]{style="background-color: yellow"}

AGCM and MkCatchParam compilation {#sec:CatParam}
=================================

The full source code
`/data/leuven/314/vsc31402/src_code/GEOS5_AGCMparam/H54p3_v24C05 _GOSWIM_zerodiff_KUL`
(`cvs co -r H54p3_v24C05_GOSWIM_zerodiff GEOSagcm`) contains the full
AGCM and routines to create parameters for this system (and
consequently, also for LDASsa).

\(1) To compile the entire AGCM, we leverage the patched `g5_modules`
from LDASsa to benefit from a working set of libraries on the HPC:

    $ cd [/xxx/]GEOS5_AGCMparam/H54p3_v24C05_GOSWIM_zerodiff_KUL
    $ set WORKDIR=`pwd`
    $ setenv ESMADIR $WORKDIR/GEOSagcm
    $ cp /data/leuven/314/vsc31402/src_code/GEOS5_LDASsa/LDASsa_m3-16_0_p1_KUL_newtopo/...
           ...LDASsa/src/g5_modules $ESMADIR/src/
    $ source $ESMADIR/src/g5_modules
    $ cd $ESMADIR/src/
    $ gmake install (./parallel_build.csh)

csh (or tcsh) is used here because the g5\_module script was written
originally in csh. , but I have not had the chance to update the latter
script. The compilation with `$ gmake install` takes long. As of today,
the code was not yet compiled entirely. However, a parallel\_build on
discover also gave errors towards the end\... Work in progress.

\(2) Instead of compiling the entire AGCM, we could just compile
mkCatchParam, to create parameters for CLSM. We again leverage the
patched `g5_modules` from LDASsa and also link and the
`Config/`-directory from LDASsa (if that of the AGCM is not in place,
e.g. if the compilation above is not done):

    $ csh
    $ cd [/xxx/]GEOS5_AGCMparam/H54p3_v24C05_GOSWIM_zerodiff_KUL
    $ set WORKDIR=`pwd`
    $ setenv ESMADIR $WORKDIR/GEOSagcm
    $ cp /data/leuven/314/vsc31402/src_code/GEOS5_LDASsa/LDASsa_m3-16_0_p1_KUL_newtopo/...
          ...LDASsa/src/g5_modules $ESMADIR/src/
    $ ln -s /data/leuven/314/vsc31402/src_code/GEOS5_LDASsa/LDASsa_m3-16_0_p1_KUL_newtopo/...
          ...LDASsa/Linux/Config $WORKDIR/GEOSagcm/
    $ source $ESMADIR/src/g5_modules
    $ cd $ESMADIR/src/GEOSgcs_GridComp/GEOSgcm_GridComp/GEOSagcm_GridComp/...
          ...GEOSphysics_GridComp/GEOSsurface_GridComp/Shared/Raster/src/
    $ [possibly, see below--] cp /data/leuven/314/vsc31402/src_code/...
          ...AGCMparam_from_Sarith_20170717_KUL/rmTinyCatchParaMod.F90 .
    $ [possibly, see below--] cp /data/leuven/314/vsc31402/src_code/...
          ...AGCMparam_from_Sarith_20170717_KUL/mod_process_hres_data.F90 .
    $ gmake

The resulting executables will be here:

    $ cd $ESMADIR/src/GEOSgcs_GridComp/GEOSgcm_GridComp/GEOSagcm_GridComp/...
          ...GEOSphysics_GridComp/GEOSsurface_GridComp/Shared/Raster/bin

Make sure to link the absolute (not relative) path of the Config-files.

2018-10-31
----------

Update path to raw data LandBCs\_files\_for\_mkCatchParam from which
parameters should be made:

    $ cd $ESMADIR/src/GEOSgcs_GridComp/GEOSgcm_GridComp/GEOSagcm_GridComp/...
          ...GEOSphysics_GridComp/GEOSsurface_GridComp/Shared/Raster/
    $ vi GNUmakefile

    replaced:
    LAND_SURFACE_DATA  = /archive/leuven/arc_00024/l_data/model_param/LandBCs_files_for_mkCatchParam/V001/

    $ vi make_bcs

    replaced (important for maskfile):
    GLOBAL_CATCH_DATA = /archive/leuven/arc_00024/l_data/model_param/LandBCs_files_for_mkCatchParam/V001/xxxxxxxxxxxx.xxx.xx

    $ cd $ESMADIR/src/GEOSgcs_GridComp/GEOSgcm_GridComp/GEOSagcm_GridComp/...
          ...GEOSphysics_GridComp/GEOSsurface_GridComp/Shared/Raster/src/

    $ vi
    mkCatchParam.F90,
    mkLISTilesPara.F90,
    mkSMAPTilesPara.F90,
    mkSMAPTilesPara_v2.F90,
    loss_during_day.f90,
    loss_surf_5cm_gensoil.f90
    \%s/\/discover\/nobackup\/projects\/gmao\/ssd\/land\/l_data\/LandBCs_files_for_mkCatchParam\/V001/\/archive\/leuven\/arc_00024\/l_data\/model_param\/LandBCs_files_for_mkCatchParam/g
    $ cd ..
    $ ./make_bcs [NOT gmake!; gmake is done inside make_bcs and takes input from it; compile via putty (not login node)]

To run parameters on the EASE-grid, you need to replace and update some
files in the source code!

mkCatchParam run {#sec:CatParam}
================

The parameters are created at any resolution or projection of choice,
based on base-files stored in
`/staging/leuven/stg_00024/l_data/model_param/LandBCs_files_for_mkCatchParam/V001/`.

\(1) To create the parameters entirely from scratch, do

    $ cd $ESMADIR/src/GEOSgcs_GridComp/GEOSgcm_GridComp/GEOSagcm_GridComp/...
          ...GEOSphysics_GridComp/GEOSsurface_GridComp/Shared/Raster/
    $ [optional--] edit to run in open_mp (GDL notebook p.92 left)
    $ ./make_bcs
    $   a) choose atmospheric grid, e.g.:
    $     m9  (EASEv2_M09)
    $     m36 (EASEv2_M36)
    $     d   (0.5x0.625 deg)
    $   b) choose landmask:
    $     O1 (capital letter O, 1)
    $   c) Y: run in background

See for example
`/staging/leuven/stg_00024/l_data/model_param/geos5/bcs/ mkCatchParam_SMAP_L4SM_v001/README_make_bcs_SMAP_L4SM_v001`.

\(2) In
`/data/leuven/314/vsc31402/src_code/AGCMparam_from_Sarith_20170717_KUL`,
a script (`create_bcs.csh`, `PBS_create_bcs.pbs`) is provided by Sarith
to re-create soil-related parameters *only* and update them. The script
is rough and needs to be edited if other soil classes than peat need to
change. The topography or vegetation parameters are kept unchanged.

-   when compiling the mkCatchParam `[see above–]`, do copy an updated
    version of `rmTinyCatchParaMod.F90` and `mod_process_hres_data.F90`
    into the `src` so that existing parameters are read from a previous
    run, rather than re-created.

-   reproduce existing parameters: CTRL: edit `create_bcs.csh` to
    \"`# goto SKIP_STEP3`\" (i.e. put in comments)

-   produce new soil-related CLSM parameters: SEN01 (SEN02, $\dots$):

    -   edit `create_bcs.csh`: give your experiment a name (SEN01) and
        edit \"`goto SKIP_STEP3`\" (i.e. remove comment)

    -    $ cp  /staging/leuven/stg_00024/l_data/model_param/...
                LandBCs_files_for_mkCatchParam/V001/Woesten_SoilParam/...
                Soil_param_100_mineral_3_OC_026_046_112_Woesten_topsoil.txt ./SEN01/ 

    -   edit the soil parameters in the previous file for the soil class
        of interest (e.g. peat)

You could launch `create_bcs.csh` on the prompt, but it will take too
long and time out on the HPC login-nodes. Therefore, launch it in the
queue:

    $ qsub PBS_create_bcs.pbs

Instructions and background are recorded in
`/data/leuven/314/vsc31402/src_code/ AGCMparam_from_Sarith_20170717_KUL/README_GDL`.
The SEN01 experiments will take an hour longer than the CTRL run (on
nccs discover), because the Richards' equation solver will be run on
peat before running mkCatchParam.

See earlier document versions to find more about this\...

GRACE(+SMOS) LDASsa {#sec:GRACESMOSDA}
===================

Compilation & run are mostly as described above for LDASsa. Please
consult the notes by Manuela Girotto for more details. Below are some
tips and tricks needed to get the GRACE-DA work fine, and possibly
update the typical GRACE+SMOS Tb DA to GRACE+SMOS SM DA:

-   `update_type .eq. 10`: GRACE DA only, `update_type .eq. 11`:
    GRACE+SMOS DA

-   `/src/Applications/LDAS_App/clsm_ensdrv_main.F90` SMOS Tb or SM in
    update\_type 11: hardwired in code,
    `update_type_SMOS = 2 !for SM, or 8 for Tb`

-   `/src/Applications/LDAS_App/clsm_ensdrv_main.F90` :
    `logical :: fwd_run = .true.`

On thinking-nodes:

-   Remember that we cannot compile on nx1/2; we need to compile via
    putty or on the groupnode.

-   Remember that we cannot use ldsetup to launch something into the
    queue from the groupnode.
